{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/christopher/Desktop/projects/Metis_Projects/Project_4/data/train-v2.0.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary of wiki articles and their respective content presented as one long string\n",
    "page = {}\n",
    "\n",
    "# dictionary of questions pertaining to specific wiki page\n",
    "qa = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_content(data):\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        page[str(i)] = dict([(\"title\", None), (\"content\", [])])\n",
    "        qa[str(i)] = dict([(\"questions\", []), (\"answers\",[])])\n",
    "\n",
    "\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        page[str(i)][\"title\"] = data[\"data\"][i][\"title\"]\n",
    "        for j in data[\"data\"][i][\"paragraphs\"]:\n",
    "            page[str(i)][\"content\"].append(j[\"context\"])\n",
    "            for k in range(len(j[\"qas\"])):\n",
    "                qa[str(i)][\"questions\"].append(j[\"qas\"][k][\"question\"])\n",
    "                \n",
    "    # save unconcatenated paragraphs for each wiki page\n",
    "    pickle_out = open('unconcat_paragraphs.pickle','wb')\n",
    "    pickle.dump(page, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    # concatenate all paragraphs for a wiki page into 1 long string\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        page[str(i)][\"content\"] = ' '.join(page[str(i)][\"content\"])\n",
    "                                                                          \n",
    "    # save content & questions\n",
    "    json.dump(page, open('wiki_pages.json','w'))\n",
    "    json.dump(qa, open('wiki_questions.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_content(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create corpus of all wiki pages content\n",
    "corpus_titles = []\n",
    "corpus = []\n",
    "for i in range(len(page)):\n",
    "    corpus_titles.append(page[str(i)][\"title\"])\n",
    "    corpus.append(page[str(i)][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 442 \n",
      " corpus_length:  442 \n",
      " corpus_title_length: 442\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "number_of_docs = len(page)\n",
    "corpus_length = len(corpus)\n",
    "corpus_titles_length = len(corpus_titles)\n",
    "print('number of docs:', number_of_docs, '\\n', 'corpus_length: ', corpus_length, '\\n', 'corpus_title_length:', corpus_titles_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_wiki_indices(arr, n):\n",
    "\n",
    "    ordered_cs = {}\n",
    "    # select the inputed question's cosine similarity array\n",
    "    cs_q = arr[-1]\n",
    "    \n",
    "    # delete cosine similarity value of 1 (the question itself)\n",
    "    cs_q = np.delete(cs_q,-1)\n",
    "\n",
    "\n",
    "    # get indices of top n wikis based on highest cosine similarity score\n",
    "    wiki_indices = np.argpartition(cs_q, -n)[-n:]\n",
    "    wiki_indices_list = list(wiki_indices)\n",
    "    \n",
    "    wiki_values_list = list(cs_q[wiki_indices])\n",
    "    \n",
    "    # create dictionary with index of cs value as keys, and actual cs value for values \n",
    "    index_cs_values = dict(zip(wiki_indices_list, wiki_values_list))\n",
    "\n",
    "    # created ordered dictionary (highest to lowest cs values)\n",
    "    for key,value in sorted(index_cs_values.items(), key=lambda item:(item[1],item[0]), reverse=True):\n",
    "        ordered_cs[key]=value\n",
    "    \n",
    "    return list(ordered_cs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_wiki(own_question=False):\n",
    "    \n",
    "    X_corpus = corpus\n",
    "    X_corpus_titles = corpus_titles\n",
    "    #print(len(X_corpus_titles), len(X_corpus))\n",
    "    \n",
    "    # choose a question & add it to the corpus\n",
    "    if own_question==True:\n",
    "        own_question = input('Ask a quesiton: ')\n",
    "        X_corpus.append(own_question)\n",
    "    \n",
    "    else:\n",
    "        wiki_num = random.randint(0,len(qa))\n",
    "        chosen_q = random.choice(qa[str(wiki_num)][\"questions\"])\n",
    "        X_corpus.append(chosen_q)\n",
    "        #X_corpus_titles.append('placeholder')\n",
    "        \n",
    "        print(chosen_q)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(strip_accents='ascii', stop_words='english')\n",
    "    X_tfidf = tfidf.fit_transform(X_corpus).toarray()\n",
    "    \n",
    "    # convert array of documents & tfidf values to dataframe\n",
    "    df = pd.DataFrame(X_tfidf, columns=tfidf.get_feature_names())\n",
    "    \n",
    "    # compute cosine similarity\n",
    "    cs_array = cosine_similarity(df)\n",
    "    \n",
    "    # Find which wiki page corresponds highest to question asked by finding\n",
    "    # index of highest value in the question's cosine similarity array.\n",
    "    # Index in cs_array is mapped 1 to 1 with the index of article titles in X_corpus_titles\n",
    "    best_wiki = X_corpus_titles[np.argmax(np.delete(cs_array[-1],-1))]\n",
    "    \n",
    "    # remove appended question to maintain clean corpus\n",
    "    del X_corpus[-1]\n",
    "    \n",
    "    print('Most Relevant Wiki Article: ', best_wiki)\n",
    "    \n",
    "    # get indices of top n wiki articles\n",
    "    top_wikis = top_wiki_indices(cs_array, 5)\n",
    "\n",
    "    # find top n wiki articles\n",
    "    top_wikis_list = []\n",
    "    \n",
    "    for index in top_wikis:\n",
    "        top_wikis_list.append(X_corpus_titles[index])\n",
    "        \n",
    "    # get first few paragraphs of top wiki article\n",
    "    top_wiki_index = top_wikis[0]\n",
    "\n",
    "    # load unconcatenated paragraphs dictionary\n",
    "    pickle_in = open('unconcat_paragraphs.pickle','rb')\n",
    "    paragraphs = pickle.load(pickle_in)\n",
    "\n",
    "    few_paragraphs = []\n",
    "    \n",
    "    for num in range(3):\n",
    "        few_paragraphs.append(paragraphs[str(top_wiki_index)][\"content\"][num])\n",
    "\n",
    "        \n",
    "    # best_wiki\n",
    "    return top_wikis_list, '\\n', '\\n', '\\n', few_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately how close did the French incursion into Leoben come to reaching Vienna?\n",
      "Most Relevant Wiki Article:  Franco-Prussian_War\n",
      "(['Franco-Prussian_War',\n",
      "  'Napoleon',\n",
      "  'Seven_Years%27_War',\n",
      "  'Crimean_War',\n",
      "  'Alsace'],\n",
      " '\\n',\n",
      " '\\n',\n",
      " '\\n',\n",
      " ['The German states proclaimed their union as the German Empire under the '\n",
      "  'Prussian king, Wilhelm I, uniting Germany as a nation-state. The Treaty of '\n",
      "  'Frankfurt of 10 May 1871 gave Germany most of Alsace and some parts of '\n",
      "  'Lorraine, which became the Imperial territory of Alsace-Lorraine '\n",
      "  '(Reichsland Elsa√ü-Lothringen).The German conquest of France and the '\n",
      "  'unification of Germany upset the European balance of power, that had '\n",
      "  'existed since the Congress of Vienna in 1815 and Otto von Bismarck '\n",
      "  'maintained great authority in international affairs for two decades. French '\n",
      "  'determination to regain Alsace-Lorraine and fear of another Franco-German '\n",
      "  'war, along with British apprehension about the balance of power, became '\n",
      "  'factors in the causes of World War I.',\n",
      "  'The Ems telegram had exactly the effect on French public opinion that '\n",
      "  'Bismarck had intended. \"This text produced the effect of a red flag on the '\n",
      "  'Gallic bull\", Bismarck later wrote. Gramont, the French foreign minister, '\n",
      "  'declared that he felt \"he had just received a slap\". The leader of the '\n",
      "  'monarchists in Parliament, Adolphe Thiers, spoke for moderation, arguing '\n",
      "  'that France had won the diplomatic battle and there was no reason for war, '\n",
      "  'but he was drowned out by cries that he was a traitor and a Prussian. '\n",
      "  \"Napoleon's new prime minister, Emile Ollivier, declared that France had \"\n",
      "  'done all that it could humanly and honorably do to prevent the war, and '\n",
      "  'that he accepted the responsibility \"with a light heart.\" A crowd of '\n",
      "  '15‚Äì20,000 people, carrying flags and patriotic banners, marched through the '\n",
      "  'streets of Paris, demanding war. On 19 July 1870 a declaration of war was '\n",
      "  'sent to the Prussian government. The southern German states immediately '\n",
      "  'sided with Prussia.',\n",
      "  'The army was still equipped with the Dreyse needle gun of Battle of '\n",
      "  'K√∂niggr√§tz fame, which was by this time showing the age of its 25-year-old '\n",
      "  'design. The rifle had a range of only 600 m (2,000 ft) and lacked the '\n",
      "  'rubber breech seal that permitted aimed shots. The deficiencies of the '\n",
      "  'needle gun were more than compensated for by the famous Krupp 6-pounder (3 '\n",
      "  'kg) steel breech-loading cannons being issued to Prussian artillery '\n",
      "  'batteries. Firing a contact-detonated shell, the Krupp gun had a longer '\n",
      "  'range and a higher rate of fire than the French bronze muzzle loading '\n",
      "  'cannon, which relied on faulty time fuses.'])\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(best_wiki())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
