{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d69a66-02fc-41e1-9038-301c0431b8c8",
   "metadata": {},
   "source": [
    "# Problem #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf98ced-24c6-4b7d-9418-0c4476b78674",
   "metadata": {},
   "source": [
    "The environment in the Nim learning model consists of piles of items. Each pile has the initial state of a certain number of items. Players alternate turns removing items from a single pile of thier choice. The winner is the player that removes the last item. Thus the environment can be seen as a series of states (items in each pile) and actions taken (how many items taken from a pile)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a67ac7-66e8-4d9d-8f0d-c18e762bcf90",
   "metadata": {},
   "source": [
    "# Problem #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156aa55-9acb-4123-8225-be45d8a75b96",
   "metadata": {},
   "source": [
    "The agents are the players who take turns removing items from the piles. Each agent's strategy differs.\n",
    "\n",
    "**Random**: This agent picks a random pile and removes a random number of items from that pile. The probabilites of which follow a unifirm distribution.\n",
    "\n",
    "**Guru**: This agent always takes the optimal action for the current state. It follows a mathematical formula which guarantees the best decision. The formula is that the guru agent takes items from any pile that will result in the binary sum of the piles' items to equal 0. If this agent goes against another Guru agent, then the one that goes first has a higher chance of winning as it will dictate the game states. This agent utilizes exploitation completely.\n",
    "\n",
    "**Q-Learner**: This agent utilizes a combination of exploitation and exploration actions. It refers to a Q-table where one axis represents all the possible states, and the other axis all the possible actions. It bases its action on its assigned policy at the time. If it is seeking to exploit, then it will find the state-action pair in the Q-table with the highest value and perform that action. With each action taken, the Q-value will be updated based on hyperparemters of alpha, a learning rate, gamma, discount rate, and a reward value R for the particular action. The exploration policy allows the Q-learner to not always take the action with the highest Q-value for a particular state. By explorating other actions for particular states, it allows the model the chance to learn quicker. The chance of exploration can be tuned with hyperparameter epsilon. As more iterations pass, the model can reduce exploration and take on more a more exploitive approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39a12c-57a9-4d51-b910-16e9ce9108ef",
   "metadata": {},
   "source": [
    "# Problem #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a652fd3-5586-4b78-8c9d-1a4ce7479ec6",
   "metadata": {},
   "source": [
    "Regarding the Q-learner model for the Nim game, each action taken has a defined reward. Reward points is added to the Q-value for a state action pair depending on the learning rate and the discount rate in the Bellman equaiton. With a high discount rate (closer to 1), an emphasis is placed on future actions. A discount rate close to zero discourages future actions. This is representative of a form of punishment or prevention of taking certain actions. The learning rate will affect the adoption of new actions. A low learning rate will cause the model to ignore new actions, and stick with the current action for a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdcb33-2066-466f-845a-1b72ab2555ef",
   "metadata": {},
   "source": [
    "# Problem #4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2df11-2643-4313-b061-dcabcb10579f",
   "metadata": {},
   "source": [
    "10 items per pile\n",
    "3 piles\n",
    "\n",
    "Take into account an empty pile, so each pile can have 0 through 10 items.\n",
    "\n",
    "Thus **total states** = 11 x 11 x 11 = **1331**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40203ee2-e3a8-4066-b9c1-cbb1eb4cdcc0",
   "metadata": {},
   "source": [
    "# Problem #5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b33071-63fb-4c7b-90bc-46ebfade4231",
   "metadata": {},
   "source": [
    "Each turn, a player can only choose 1 pile to remove a certain number of items.\n",
    "\n",
    "The player must remove at least 1 item, out of a max of 10 items in a pile. So for each pile a player has 10 unique actions (remove 1 item, remove 2 items, ...).\n",
    "\n",
    "For 3 piles, a total of **30 unique actions** are possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ca7e0-4dc0-4df7-94fa-bf63e7dbb6f0",
   "metadata": {},
   "source": [
    "# Problem #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22737ea8-3d38-4a95-b528-03c75d60d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f029afa-799a-405d-bc36-e85b6b3cdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8ba21f-2abf-4330-8390-df0436c05e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s}{wins['A']:5d}  {b:>8s}{wins['B']:5d}\")\n",
    "\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e27dea-7466-4269-8686-fd6dfa9632a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,   Random  515    Random  485\n",
      "1000 games,     Guru  998    Random    2\n",
      "1000 games,   Random   11      Guru  989\n",
      "1000 games,     Guru  938      Guru   62\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Random', 'Random')\n",
    "play_games(1000, 'Guru', 'Random')\n",
    "play_games(1000, 'Random', 'Guru')\n",
    "play_games(1000, 'Guru', 'Guru') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f20d4fd-a243-4601-9e97-2b7fd71d685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1321cdc3-a295-4470-b6d4-3335f4757dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  504    Random  496\n",
      "1000 games, Qlearner  591    Random  409\n",
      "1000 games, Qlearner  624    Random  376\n",
      "1000 games, Qlearner  688    Random  312\n",
      "1000 games, Qlearner  714    Random  286\n",
      "1000 games, Qlearner  709    Random  291\n",
      "1000 games, Qlearner  741    Random  259\n"
     ]
    }
   ],
   "source": [
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'Qlearner', 'Random')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aad3115-c63b-4169-a4ed-9df6a433cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.504, 0.591, 0.624, 0.688, 0.714, 0.709, 0.741]\n"
     ]
    }
   ],
   "source": [
    "print(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3ce554-a154-402a-ad2e-7d88f043e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner    3      Guru  997\n",
      "1000 games, Qlearner    1      Guru  999\n",
      "1000 games, Qlearner    9      Guru  991\n",
      "1000 games, Qlearner   17      Guru  983\n",
      "1000 games, Qlearner   29      Guru  971\n",
      "1000 games, Qlearner   28      Guru  972\n",
      "1000 games, Qlearner   23      Guru  977\n"
     ]
    }
   ],
   "source": [
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'Qlearner', 'Guru')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28fb7a-700a-4136-904f-233eb2da3b1d",
   "metadata": {},
   "source": [
    "As expected, the guru out-performs Qlearner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75aab6b-d109-461f-a9b1-372bd58f247c",
   "metadata": {},
   "source": [
    "Below: **Modifications to Q-learner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e7d71e-4e82-40a3-8fae-aabb34c502f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gamma to 0\n",
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.0, 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f8835-49ab-4a8a-9c80-d37a4999dd90",
   "metadata": {},
   "source": [
    "nim_qlearn(100000)\n",
    "play_games(1000, 'Qlearner', 'Random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a4022-c5af-428a-89a0-3ecd1d8327c5",
   "metadata": {},
   "source": [
    "By setting gamma to 0, the Qlearner improved in performance against the random player from .705 to .801\n",
    "\n",
    "With gamma=0, it tries to maximize the current reward for an action at a certain state. It does not allow for the propogation of future rewards onto the current state-action pair.\n",
    "\n",
    "In the above Qlearning model, it is exploring 100% of the time until it finishes the game. By playing a high number of games it allows for the model to approach towards the right Q values.\n",
    "\n",
    "Thus, the increased performance in gamma=0 can be attributed to reducing the fluctuation in the updates of the Q values. However, this leads to \"overfitting\" the Q values and mat not lead to a more generalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d611dc41-fda6-4384-99b3-52e3338fcfbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103624/4185929252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# observe effect vs guru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay_games\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Qlearner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Guru'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_103624/3930512481.py\u001b[0m in \u001b[0;36mplay_games\u001b[0;34m(_n, a, b)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mwins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{_n} games, {a:>8s}{wins['A']:5d}  {b:>8s}{wins['B']:5d}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_103624/3930512481.py\u001b[0m in \u001b[0;36mgame\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mside\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'A'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mEngines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# print(state, move, pile)  # debug purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpile\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_103624/2210656492.py\u001b[0m in \u001b[0;36mnim_qlearner\u001b[0;34m(_st)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnim_qlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_st\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# pick the best rewarding move, equation 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# exploitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# index is based on move, pile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mITEMS_MX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mITEMS_MX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# observe effect vs guru\n",
    "play_games(1000, 'Qlearner', 'Guru')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70fa68-fcc4-4aac-a232-965c53a076c7",
   "metadata": {},
   "source": [
    "The gamma=0 results in a worse performance against the guru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ca196-3d21-4edc-b5d1-a50083d3fdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
